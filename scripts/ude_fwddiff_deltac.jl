using OrdinaryDiffEq
using Random
using LinearAlgebra, Statistics,Lux
rng = Xoshiro(123)
using Bolt
using Plots
using Optimization,SciMLSensitivity,OptimizationOptimisers,ComponentArrays,OptimizationOptimJL
using AbstractDifferentiation
import AbstractDifferentiation as AD, ForwardDiff

# setup ks won't use all of them here...
L=2f3
lkmi,lkmax,nk = log10(2.0f0*œÄ/L),log10(0.2f0),8
kk = 10.0f0.^(collect(lkmi:(lkmax-lkmi)/(nk-1):lkmax))
‚Ñì·µß=15
pertlen=2(‚Ñì·µß+1)+5

# define network
U = Lux.Chain(Lux.Dense(pertlen+1, 8, tanh), #input is u,t
                  Lux.Dense(8, 8,tanh),
                  Lux.Dense(8, 2))
p, st = Lux.setup(rng, U)

# copy the hierarchy function os it works for nn - for this to work you need the Hierarchy_nn struct and unpack_nn in perturbations.jl
function hierarchy_nn!(du, u, hierarchy::Hierarchy_nn{T, BasicNewtonian}, x)  where T
    # compute cosmological quantities at time x, and do some unpacking
    k, ‚Ñì·µß, par, bg, ih = hierarchy.k, hierarchy.‚Ñì·µß, hierarchy.par, hierarchy.bg, hierarchy.ih
    Œ©_r, Œ©_b, Œ©_c, H‚ÇÄ¬≤ = par.Œ©_r, par.Œ©_b, par.Œ©_c, bg.H‚ÇÄ^2 
    ‚Ñã‚Çì, ‚Ñã‚Çì‚Ä≤, Œ∑‚Çì, œÑ‚Çì‚Ä≤, œÑ‚Çì‚Ä≤‚Ä≤ = bg.‚Ñã(x), bg.‚Ñã‚Ä≤(x), bg.Œ∑(x), ih.œÑ‚Ä≤(x), ih.œÑ‚Ä≤‚Ä≤(x)
    a = x2a(x)
    R = 4Œ©_r / (3Œ©_b * a)
    csb¬≤ = ih.csb¬≤(x)

    # the new free cdm index (not used here)
    Œ±_c = par.Œ±_c
    # get the nn params
    p_nn = hierarchy.p

    Œò, Œò·µñ, Œ¶, Œ¥_c, v_c,Œ¥_b, v_b = unpack_nn(u, hierarchy)  
    Œò‚Ä≤, Œò·µñ‚Ä≤, _, _, _, _, _ = unpack_nn(du, hierarchy) 

    # Here I am throwing away the neutrinos entriely, which is probably bad
    Œ® = -Œ¶ - 12H‚ÇÄ¬≤ / k^2 / a^2 * (Œ©_r * Œò[2]
                                #   + Œ©_c * a^(4+Œ±_c) * œÉ_c  #ignore this
                                  )

    Œ¶‚Ä≤ = Œ® - k^2 / (3‚Ñã‚Çì^2) * Œ¶ + H‚ÇÄ¬≤ / (2‚Ñã‚Çì^2) * (
        Œ©_c * a^(2+Œ±_c) * Œ¥_c
        + Œ©_b * a^(-1) * Œ¥_b
        + 4Œ©_r * a^(-2) * Œò[0]
        )

    # matter
    nnin = hcat([u...,x])
    uÃÇ = U(nnin,p_nn,st)[1]
    Œ¥‚Ä≤ = uÃÇ[1]
    v‚Ä≤ = uÃÇ[2]
    # here we implicitly assume œÉ_c = 0

    Œ¥_b‚Ä≤ = k / ‚Ñã‚Çì * v_b - 3Œ¶‚Ä≤
    v_b‚Ä≤ = -v_b - k / ‚Ñã‚Çì * ( Œ® + csb¬≤ *  Œ¥_b) + œÑ‚Çì‚Ä≤ * R * (3Œò[1] + v_b)
    # photons
    Œ† = Œò[2] + Œò·µñ[2] + Œò·µñ[0]
    Œò‚Ä≤[0] = -k / ‚Ñã‚Çì * Œò[1] - Œ¶‚Ä≤
    Œò‚Ä≤[1] = k / (3‚Ñã‚Çì) * Œò[0] - 2k / (3‚Ñã‚Çì) * Œò[2] + k / (3‚Ñã‚Çì) * Œ® + œÑ‚Çì‚Ä≤ * (Œò[1] + v_b/3)
    for ‚Ñì in 2:(‚Ñì·µß-1)
        Œò‚Ä≤[‚Ñì] = ‚Ñì * k / ((2‚Ñì+1) * ‚Ñã‚Çì) * Œò[‚Ñì-1] -
            (‚Ñì+1) * k / ((2‚Ñì+1) * ‚Ñã‚Çì) * Œò[‚Ñì+1] + œÑ‚Çì‚Ä≤ * (Œò[‚Ñì] - Œ† * Bolt.Œ¥_kron(‚Ñì, 2) / 10)
    end
    Œò·µñ‚Ä≤[0] = -k / ‚Ñã‚Çì * Œò·µñ[1] + œÑ‚Çì‚Ä≤ * (Œò·µñ[0] - Œ† / 2)
    for ‚Ñì in 1:(‚Ñì·µß-1)
        Œò·µñ‚Ä≤[‚Ñì] = ‚Ñì * k / ((2‚Ñì+1) * ‚Ñã‚Çì) * Œò·µñ[‚Ñì-1] -
            (‚Ñì+1) * k / ((2‚Ñì+1) * ‚Ñã‚Çì) * Œò·µñ[‚Ñì+1] + œÑ‚Çì‚Ä≤ * (Œò·µñ[‚Ñì] - Œ† * Œ¥_kron(‚Ñì, 2) / 10)
    end
    Œò‚Ä≤[‚Ñì·µß] = k / ‚Ñã‚Çì * Œò[‚Ñì·µß-1] - ( (‚Ñì·µß + 1) / (‚Ñã‚Çì * Œ∑‚Çì) - œÑ‚Çì‚Ä≤ ) * Œò[‚Ñì·µß]
    Œò·µñ‚Ä≤[‚Ñì·µß] = k / ‚Ñã‚Çì * Œò·µñ[‚Ñì·µß-1] - ( (‚Ñì·µß + 1) / (‚Ñã‚Çì * Œ∑‚Çì) - œÑ‚Çì‚Ä≤ ) * Œò·µñ[‚Ñì·µß]
    du[2(‚Ñì·µß+1)+1:2(‚Ñì·µß+1)+5] .= Œ¶‚Ä≤, Œ¥‚Ä≤, v‚Ä≤, Œ¥_b‚Ä≤, v_b‚Ä≤
    return nothing
end

# use only the longest k mode
function hierarchy_nnu!(du, u, p, x) 
    hierarchy = Hierarchy_nn(BasicNewtonian(), ùï°test, bgtest, ihtest, kk[1],p,15);
    hierarchy_nn!(du, u, hierarchy, x)
end

#log loss
function loss(Œ∏)
    XÃÇ = predict(Œ∏)
    log(mean(abs2, Ytrain_ode .- XÃÇ) )
end


# adtype = Optimization.AutoZygote()
adtype = Optimization.AutoForwardDiff()
optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, ComponentVector(p))


# some setup
tspan = (-20.0f0, 0.0f0)
ùï°test = CosmoParams(Œ©_c=0.3,Œ±_c=-3.0);
bgtest = Background(ùï°test; x_grid=-20.0f0:1f-1:0.0f0);
ihtest = Bolt.get_saha_ih(ùï°test, bgtest);
hierarchytest = Hierarchy(BasicNewtonian(), ùï°test, bgtest, ihtest, kk[1],15);
hierarchytestnn = Hierarchy_nn(BasicNewtonian(), ùï°test, bgtest, ihtest, kk[1], ComponentArray(p),15);
u0 = Bolt.initial_conditions_nn(tspan[1],hierarchytestnn)

# problem for truth and one we will remake
prob_trueode = ODEProblem(Bolt.hierarchy!, u0, tspan,hierarchytest)
prob_nn = ODEProblem(hierarchy_nnu!, u0, (bgtest.x_grid[1],bgtest.x_grid[end]), ComponentArray{Float64}(p))

# Generate some noisy data (at reduced solver tolerance)
ode_data = Array(solve(prob_trueode, KenCarp4(), saveat = bgtest.x_grid,
                abstol = 1e-3, reltol = 1e-3))
Œ¥_true,v_true = ode_data[end-3,:],ode_data[end-2,:]
œÉfakeode = 0.1
noise_fakeode = Œ¥_true .*  randn(rng,size(Œ¥_true)).*œÉfakeode
noise_fakeode_v = v_true .*  randn(rng,size(v_true)).*œÉfakeode
Ytrain_ode = hcat([Œ¥_true .+ noise_fakeode,v_true .+ noise_fakeode_v]...)

# NB I dropped the "Float64" type argument to the ComponentArray, maybe we should put it back?
function predict(Œ∏, T = bgtest.x_grid)
    _prob = remake(prob_nn, u0 = u0, tspan = (T[1], T[end]), p =  Œ∏)
    res = Array(solve(_prob, KenCarp4(), saveat = T,
                abstol = 1e-3, reltol = 1e-3))
    return hcat(res[end-3,:],res[end-2,:])
end

# test the prediction gradient
ab = AD.ForwardDiffBackend()
AD.jacobian(ab,predict,ComponentArray(p))


# Training
losses = [];
callback = function (p, l)
    push!(losses, l)
    if length(losses) % 5 == 0
    println("Current loss after $(length(losses)) iterations: $(losses[end])")
    end
    return false
end
niter_1,niter_2,niter_3 =50,50,20
Œ∑_1,Œ∑_2,Œ∑_3 = 1.0,0.1,0.01
res1 = Optimization.solve(optprob, ADAM(1.0), callback = callback, maxiters = niter_1)
# this is pretty slow, on the order of half an hour, but it is at least running!
# Now idk what is wrong with reverse mode...but getting errors about typing...
#Current loss after 50 iterations: 3.8709838260084415
# Get the result
test_predict_o1 = predict(res1.u)

optprob2 = remake(optprob,u0 = res1.u);
res2 = Optimization.solve(optprob2, ADAM(Œ∑_2), callback = callback, maxiters = niter_2)
test_predict_o2 = predict(res2.u)
#Current loss after 100 iterations: 1.1319215191941459

optprob3 = remake(optprob2,u0 = res2.u);
res3 = Optimization.solve(optprob3, ADAM(Œ∑_3), callback = callback, maxiters = niter_3)
test_predict_o3 = predict(res3.u)
#Current loss after 120 iterations: 1.0862281116475199

# FIXME MORE INVOLVED ADAM SCHEDULE

# for the heck of it try BFGS, not sure what parameters it usually takes
optprob4 = remake(optprob3,u0 = res3.u);
res4 = Optimization.solve(optprob3, BFGS(), 
                            callback = callback, maxiters = 10)
test_predict_o4 = predict(res4.u)
# wow this is slow...I guess due to Hessian approximation?
# We have the gradient so idk why this takes so much longer than ADAM?
# Somehow loss actually goes up also? Maybe we overshoot, can try again with specified initial stepsize...

# Plots of the learned perturbations
Plots.scatter(bgtest.x_grid,Ytrain_ode[:,1],label="data")
Plots.plot!(bgtest.x_grid,Œ¥_true,label="truth",yscale=:log10,lw=2.5)
Plots.plot!(bgtest.x_grid,test_predict_o1[:,1],label="opt-v1",lw=2.5,ls=:dash)
Plots.plot!(bgtest.x_grid,test_predict_o4[:,1],label="opt-v1-full",lw=2.5,ls=:dot)
Plots.title!(raw"$\delta_{c}$")
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$\delta_{c}(a)$")
savefig("../plots/deltac_learning_v1_multnoise$(œÉfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Œ∑_1)_$(Œ∑_2)_$(Œ∑_3)_bfgs.png")

log10.(test_predict_o4[:,2])

Plots.scatter(bgtest.x_grid,Ytrain_ode[:,2],label="data")#,legend=:bottomright)
Plots.plot!(bgtest.x_grid,v_true,label="truth")
Plots.plot!(bgtest.x_grid,test_predict_o1[:,2],label="opt-v1")
Plots.plot!(bgtest.x_grid,test_predict_o4[:,2],label="opt-v1-full",lw=2.5,ls=:dot)
Plots.title!(raw"$v_{c}$")
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$v_{c}(a)$")
savefig("../plots/vc_learning_v1_multnoise$(œÉfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Œ∑_1)_$(Œ∑_2)_$(Œ∑_3)_bfgs.png")


function get_Œ¶‚Ä≤_Œ®(u,hierarchy::Hierarchy{T},x) where T
    k, ‚Ñì·µß, par, bg, ih = hierarchy.k, hierarchy.‚Ñì·µß, hierarchy.par, hierarchy.bg, hierarchy.ih
    Œ©_r, Œ©_b, Œ©_c, H‚ÇÄ¬≤ = par.Œ©_r, par.Œ©_b, par.Œ©_c, bg.H‚ÇÄ^2 
    ‚Ñã‚Çì, ‚Ñã‚Çì‚Ä≤, Œ∑‚Çì, œÑ‚Çì‚Ä≤, œÑ‚Çì‚Ä≤‚Ä≤ = bg.‚Ñã(x), bg.‚Ñã‚Ä≤(x), bg.Œ∑(x), ih.œÑ‚Ä≤(x), ih.œÑ‚Ä≤‚Ä≤(x)
    a = x2a(x)
    R = 4Œ©_r / (3Œ©_b * a)
    csb¬≤ = ih.csb¬≤(x)
    Œ±_c = par.Œ±_c
    Œò, Œò·µñ, Œ¶, Œ¥, v, Œ¥_b, v_b = unpack(u, hierarchy)  
    # metric perturbations (00 and ij FRW Einstein eqns)
    Œ® = -Œ¶ - 12H‚ÇÄ¬≤ / k^2 / a^2 * (Œ©_r * Œò[2]
                                  )

    Œ¶‚Ä≤ = Œ® - k^2 / (3‚Ñã‚Çì^2) * Œ¶ + H‚ÇÄ¬≤ / (2‚Ñã‚Çì^2) * (
        Œ©_c * a^(2+Œ±_c) * Œ¥ 
        + Œ©_b * a^(-1) * Œ¥_b
        + 4Œ©_r * a^(-2) * Œò[0]
        )

    return Œ¶‚Ä≤,Œ®
end

# The reconstructed function
Œ¶‚Ä≤_true,Œ®_true = zeros(length(hierarchytest.bg.x_grid)),zeros(length(hierarchytest.bg.x_grid))
nn_Œ¥‚Ä≤,nn_v‚Ä≤ = zeros(length(hierarchytest.bg.x_grid)),zeros(length(hierarchytest.bg.x_grid))
for j in 1:length(hierarchytest.bg.x_grid)
    Œ¶‚Ä≤_true[j],Œ®_true[j] = get_Œ¶‚Ä≤_Œ®(ode_data[:,j],hierarchytest,hierarchytest.bg.x_grid[j])
    nnin = hcat([ode_data[:,j]...,hierarchytest.bg.x_grid[j]])
    # nn_u‚Ä≤ = U(nnin,res1.u,st)[1]
    nn_u‚Ä≤ = U(nnin,res4.u,st)[1]
    nn_Œ¥‚Ä≤[j],nn_v‚Ä≤[j] = nn_u‚Ä≤[1], nn_u‚Ä≤[2]
end


true_Œ¥‚Ä≤ = hierarchytestnn.k ./ hierarchytestnn.bg.‚Ñã .* v_true .- 3Œ¶‚Ä≤_true
true_v‚Ä≤ = -v_true .- hierarchytestnn.k  ./ hierarchytestnn.bg.‚Ñã  .* Œ®_true

Plots.plot(hierarchytestnn.bg.x_grid, true_Œ¥‚Ä≤,label="truth",lw=2.5)
Plots.plot!(hierarchytestnn.bg.x_grid,nn_Œ¥‚Ä≤,label="nn-v1",lw=2.5)
Plots.plot!(hierarchytestnn.bg.x_grid,nn_Œ¥‚Ä≤,label="nn-v1-full",lw=2.5)
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$\delta'(a)$")
Plots.title!(raw"recon $\delta_{c}$")
savefig("../plots/deltacprime_learning_v1_multnoise$(œÉfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Œ∑_1)_$(Œ∑_2)_$(Œ∑_3)_bfgs.png")

Plots.plot(hierarchytestnn.bg.x_grid,true_v‚Ä≤,label="truth",lw=2.5)
Plots.plot!(hierarchytestnn.bg.x_grid,nn_v‚Ä≤,label="nn-v1",lw=2.5)
Plots.plot!(hierarchytestnn.bg.x_grid,nn_v‚Ä≤,label="nn-v1-full",lw=2.5)
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$v'(a)$")
Plots.title!(raw"recon $v_{c}$")
savefig("../plots/vcprime_learning_v1_multnoise$(œÉfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Œ∑_1)_$(Œ∑_2)_$(Œ∑_3)_bfgs.png")


# loss
Plots.plot(losses)
Plots.xlabel!(raw"iters")
Plots.ylabel!(raw"loss")
savefig("../plots/loss_learning_v1_multnoise$(œÉfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Œ∑_1)_$(Œ∑_2)_$(Œ∑_3)_bfgs.png")


# --------------------------------
# Old code testing AD backends

# # try swapping out non-Enzyme AD backends as Marius suggested:
# using Zygote
# Zygote.gradient(loss,p)
# Zygote.gradient(loss, ComponentArray{Float64}(p) )


# using Enzyme
# autodiff(Reverse, loss, Active, Active(p))
# autodiff(Reverse, loss, Active, Active( ComponentArray{Float64}(p) ))
# autodiff(Forward, loss, Duplicated, Const(p))


# ab = AD.ZygoteBackend()
# f(x) = log(sum(exp, x))
# AD.gradient(ab, f, rand(10))
# # Zygote
# AD.gradient(ab,loss,ComponentArray(p))


# # RevserseDiff
# import ReverseDiff
# ab = AD.ReverseDiffBackend()
# AD.gradient(ab, f, rand(10))
# # AD.gradient(ab,loss,p)
# AD.gradient(ab,loss,ComponentArray(p))

# # Tracker
# import Tracker
# ab = AD.TrackerBackend()
# AD.gradient(ab, f, rand(10))
# AD.gradient(ab,loss,p)

# # ForwardDiff
# import ForwardDiff
# ab = AD.ForwardDiffBackend()
# AD.gradient(ab, f, rand(10))
# AD.gradient(ab,loss,p)

# loss(ComponentArray(p))
# AD.gradient(ab,loss,ComponentArray(p))
# #this at least should work? why not?

# #ok so why is the gradient zero? Is predictor gradient also zero?
# AD.jacobian(ab,predict,ComponentArray(p))
# # jacobian, but yes. Why is it zero?

# AD.jacobian(ab,predict,ComponentArray(p))

# typeof(ComponentArray(p))

# # Try fwd diff optimizing...



# # FiniteDifferences
# import FiniteDifferences
# ab = AD.FiniteDifferencesBackend()
# AD.gradient(ab, f, rand(10))
# AD.gradient(ab,loss,p)

# # check that forwarddiff works because it did before...
# ForwardDiff.jacobian(loss,p)
# typeof(p)

