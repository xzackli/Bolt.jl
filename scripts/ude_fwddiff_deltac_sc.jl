using OrdinaryDiffEq
using Random
using LinearAlgebra, Statistics,Lux
rng = Xoshiro(123)
using Bolt
using Plots
using Optimization,SciMLSensitivity,OptimizationOptimisers,ComponentArrays,OptimizationOptimJL
using AbstractDifferentiation
import AbstractDifferentiation as AD, ForwardDiff
using SimpleChains
using AdvancedHMC,LogDensityProblems

# setup ks won't use all of them here...
L=2f3
lkmi,lkmax,nk = log10(2.0f0*Ï€/L),log10(0.2f0),8
kk = 10.0f0.^(collect(lkmi:(lkmax-lkmi)/(nk-1):lkmax))
â„“áµ§=15
pertlen=2(â„“áµ§+1)+5

# define network
width=8
# U = Lux.Chain(Lux.Dense(pertlen+1, width, tanh), #input is u,t
#                   Lux.Dense(width, width,tanh),
#                   Lux.Dense(width, 2))
# p, st = Lux.setup(rng, U)
function get_nn(m,d_in)
    NN = SimpleChain(static(d_in),
        TurboDense{true}(tanh, m), 
        TurboDense{true}(tanh, m), 
        TurboDense{false}(identity, 2) #have not tested non-scalar output
    );
    p = SimpleChains.init_params(NN;rng); 
    G = SimpleChains.alloc_threaded_grad(NN);
    return NN,p,G
end

NN,p,G = get_nn(width,pertlen+1)


# copy the hierarchy function os it works for nn - for this to work you need the Hierarchy_nn struct and unpack_nn in perturbations.jl

function hierarchy_nn!(du, u, hierarchy::Hierarchy_nn{T, BasicNewtonian}, x)  where T
    # compute cosmological quantities at time x, and do some unpacking
    k, â„“áµ§, par, bg, ih = hierarchy.k, hierarchy.â„“áµ§, hierarchy.par, hierarchy.bg, hierarchy.ih
    Î©_r, Î©_b, Î©_c, Hâ‚€Â² = par.Î©_r, par.Î©_b, par.Î©_c, bg.Hâ‚€^2 
    â„‹â‚“, â„‹â‚“â€², Î·â‚“, Ï„â‚“â€², Ï„â‚“â€²â€² = bg.â„‹(x), bg.â„‹â€²(x), bg.Î·(x), ih.Ï„â€²(x), ih.Ï„â€²â€²(x)
    a = x2a(x)
    R = 4Î©_r / (3Î©_b * a)
    csbÂ² = ih.csbÂ²(x)

    # the new free cdm index (not used here)
    Î±_c = par.Î±_c
    # get the nn params
    p_nn = hierarchy.p

    Î˜, Î˜áµ–, Î¦, Î´_c, v_c,Î´_b, v_b = unpack_nn(u, hierarchy)  
    Î˜â€², Î˜áµ–â€², _, _, _, _, _ = unpack_nn(du, hierarchy) 

    # Here I am throwing away the neutrinos entriely, which is probably bad
    Î¨ = -Î¦ - 12Hâ‚€Â² / k^2 / a^2 * (Î©_r * Î˜[2]
                                #   + Î©_c * a^(4+Î±_c) * Ïƒ_c  #ignore this
                                  )

    Î¦â€² = Î¨ - k^2 / (3â„‹â‚“^2) * Î¦ + Hâ‚€Â² / (2â„‹â‚“^2) * (
        Î©_c * a^(2+Î±_c) * Î´_c
        + Î©_b * a^(-1) * Î´_b
        + 4Î©_r * a^(-2) * Î˜[0]
        )

    # matter
    nnin = hcat([u...,x])
    # uÌ‚ = U(nnin,p_nn,st)[1]
    # uÌ‚ = sc_noloss(nnin,p_nn)'  .* std(true_uâ€²,dims=1) .+ mean(true_uâ€²,dims=1)
    uÌ‚ = NN(nnin,p_nn)' 
    Î´â€² = uÌ‚[1]
    vâ€² = uÌ‚[2]
    # here we implicitly assume Ïƒ_c = 0

    Î´_bâ€² = k / â„‹â‚“ * v_b - 3Î¦â€²
    v_bâ€² = -v_b - k / â„‹â‚“ * ( Î¨ + csbÂ² *  Î´_b) + Ï„â‚“â€² * R * (3Î˜[1] + v_b)
    # photons
    Î  = Î˜[2] + Î˜áµ–[2] + Î˜áµ–[0]
    Î˜â€²[0] = -k / â„‹â‚“ * Î˜[1] - Î¦â€²
    Î˜â€²[1] = k / (3â„‹â‚“) * Î˜[0] - 2k / (3â„‹â‚“) * Î˜[2] + k / (3â„‹â‚“) * Î¨ + Ï„â‚“â€² * (Î˜[1] + v_b/3)
    for â„“ in 2:(â„“áµ§-1)
        Î˜â€²[â„“] = â„“ * k / ((2â„“+1) * â„‹â‚“) * Î˜[â„“-1] -
            (â„“+1) * k / ((2â„“+1) * â„‹â‚“) * Î˜[â„“+1] + Ï„â‚“â€² * (Î˜[â„“] - Î  * Bolt.Î´_kron(â„“, 2) / 10)
    end
    Î˜áµ–â€²[0] = -k / â„‹â‚“ * Î˜áµ–[1] + Ï„â‚“â€² * (Î˜áµ–[0] - Î  / 2)
    for â„“ in 1:(â„“áµ§-1)
        Î˜áµ–â€²[â„“] = â„“ * k / ((2â„“+1) * â„‹â‚“) * Î˜áµ–[â„“-1] -
            (â„“+1) * k / ((2â„“+1) * â„‹â‚“) * Î˜áµ–[â„“+1] + Ï„â‚“â€² * (Î˜áµ–[â„“] - Î  * Î´_kron(â„“, 2) / 10)
    end
    Î˜â€²[â„“áµ§] = k / â„‹â‚“ * Î˜[â„“áµ§-1] - ( (â„“áµ§ + 1) / (â„‹â‚“ * Î·â‚“) - Ï„â‚“â€² ) * Î˜[â„“áµ§]
    Î˜áµ–â€²[â„“áµ§] = k / â„‹â‚“ * Î˜áµ–[â„“áµ§-1] - ( (â„“áµ§ + 1) / (â„‹â‚“ * Î·â‚“) - Ï„â‚“â€² ) * Î˜áµ–[â„“áµ§]
    du[2(â„“áµ§+1)+1:2(â„“áµ§+1)+5] .= Î¦â€², Î´â€², vâ€², Î´_bâ€², v_bâ€²
    return nothing
end

# use only the longest k mode
function hierarchy_nnu!(du, u, p, x) 
    hierarchy = Hierarchy_nn(BasicNewtonian(), ð•¡test, bgtest, ihtest, kk[1],p,15);
    hierarchy_nn!(du, u, hierarchy, x)
end



# some setup
tspan = (-20.0f0, 0.0f0)
ð•¡test = CosmoParams(Î©_c=0.3,Î±_c=-3.0);
bgtest = Background(ð•¡test; x_grid=-20.0f0:1f-1:0.0f0);
ihtest = Bolt.get_saha_ih(ð•¡test, bgtest);
hierarchytest = Hierarchy(BasicNewtonian(), ð•¡test, bgtest, ihtest, kk[1],15);
hierarchytestnn = Hierarchy_nn(BasicNewtonian(), ð•¡test, bgtest, ihtest, kk[1],p,15);
u0 = Bolt.initial_conditions_nn(tspan[1],hierarchytestnn)

NN(hcat([u0...,-20.f0]),p)
# NN([-20.f0],p)

# problem for truth and one we will remake
prob_trueode = ODEProblem(Bolt.hierarchy!, u0, tspan,hierarchytest)
# prob_nn = ODEProblem(hierarchy_nnu!, u0, (bgtest.x_grid[1],bgtest.x_grid[end]), ComponentArray{Float64}(p))

# Generate some noisy data (at reduced solver tolerance)
ode_data = Array(solve(prob_trueode, KenCarp4(), saveat = bgtest.x_grid,
                abstol = 1f-3, reltol = 1f-3))
Î´_true,v_true = ode_data[end-3,:],ode_data[end-2,:]
Ïƒfakeode = 0.1f0
noise_fakeode = Î´_true .*  randn(rng,size(Î´_true)).*Ïƒfakeode
noise_fakeode_v = v_true .*  randn(rng,size(v_true)).*Ïƒfakeode
Ytrain_ode = hcat([Î´_true .+ noise_fakeode,v_true .+ noise_fakeode_v]...)
noise_both_old = Float32.(hcat([noise_fakeode,noise_fakeode_v]...))
noise_both = Float32.(hcat([Î´_true.*Ïƒfakeode,v_true.*Ïƒfakeode]...))

#float conversion
fl_xgrid = Float32.(bgtest.x_grid)
fu0 = Float32.(u0)
prob_nn = ODEProblem(hierarchy_nnu!, fu0, (fl_xgrid[1],fl_xgrid[end]), p)

function predict(Î¸, T = fl_xgrid)
    _prob = remake(prob_nn, u0 = fu0, tspan = (T[1], T[end]), p =  Î¸)
    res = Array(solve(_prob, KenCarp4(), saveat = T,
                abstol = 1f-3, reltol = 1f-3))
    return hcat(res[end-3,:],res[end-2,:])
end

#log loss
function loss(Î¸)
    XÌ‚ = predict(Î¸)
    log(sum(abs2, (Ytrain_ode - XÌ‚)./ noise_both ) )
end


# adtype = Optimization.AutoZygote()
adtype = Optimization.AutoForwardDiff()
optf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)
optprob = Optimization.OptimizationProblem(optf,p)



# Training
function make_gif_plot(prediction,iter)
    p = Plots.plot()
    Plots.scatter!(p,bgtest.x_grid,Ytrain_ode[:,1],label="data")
    Plots.plot!(p,bgtest.x_grid,prediction,lw=2,label="iter = $(iter)")
    Plots.xlabel!(p,raw"$\log(a)$")
    Plots.ylabel!(p,raw"$\delta_{c}(a)$")
    savefig("../plots/learning_v1_multnoise$(Ïƒfakeode)_Adam$(iter)_s123_sc_chisq_to.png")
    p
end

losses = [];
pp =[];
callback = function (p, l)
    push!(losses, l)
    push!(pp, p)
    if length(losses) % 5 == 0
    println("Current loss after $(length(losses)) iterations: $(losses[end])")
    make_gif_plot(predict(p)[:,1],length(losses))
    end
    return false
end
niter_1,niter_2,niter_3 =50,50,20
Î·_1,Î·_2,Î·_3 = 1.f0,0.1f0,0.01f0
#loss doesn't go down very much at all with 1f-3 Î·1

res1 = Optimization.solve(optprob, ADAM(0.01), callback = callback, maxiters = niter_1)
# this is pretty slow, on the order of half an hour, but it is at least running!
#Current loss after 50 iterations: 3.8709838260084415
test_predict_o1 = predict(res1.u)

optprob2 = remake(optprob,u0 = res1.u);
res2 = Optimization.solve(optprob2, ADAM(Î·_2), callback = callback, maxiters = niter_2)
test_predict_o2 = predict(res2.u)
#Current loss after 100 iterations: 1.1319215191941459

optprob3 = remake(optprob,u0 = res2.u);
res3 = Optimization.solve(optprob3, ADAM(Î·_3), callback = callback, maxiters = 100)
test_predict_o3 = predict(res3.u)
#Current loss after 120 iterations: 1.0862281116475199

# FIXME MORE INVOLVED ADAM SCHEDULE
# for the heck of it try BFGS, not sure what parameters it usually takes
optprob4 = remake(optprob3,u0 = res4.u);
# res4 = Optimization.solve(optprob4, BFGS(), 
res4 = Optimization.solve(optprob4, ADAM(Î·_3/1000), callback = callback, 
                            maxiters = 100)
                            # callback = callback, maxiters = 10)
test_predict_o4 = predict(res4.u)
# wow this is slow...I guess due to Hessian approximation?
# We have the gradient so idk why this takes so much longer than ADAM?
# Somehow loss actually goes up also? Maybe we overshoot, can try again with specified initial stepsize...
test_predict_o1[:,1]
# Plots of the learned perturbations
Plots.scatter(bgtest.x_grid,Ytrain_ode[:,1],label="data")
Plots.plot!(bgtest.x_grid,Î´_true,label="truth",lw=2.5)#,yscale=:log10)
Plots.plot!(bgtest.x_grid,predict(pc)[:,1],label="opt-v1",lw=2.5,ls=:dash)
Plots.title!(raw"$\delta_{c}$")
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$\delta_{c}(a)$")
savefig("../plots/deltac_learning_v1_multnoise$(Ïƒfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Î·_1)_$(Î·_2)_$(Î·_3)_bfgs.png")
pc
p
log10.(test_predict_o4[:,2])


# It *SEEMS LIKE* the model isn't flexible enough - i.e. when I shift to
# weighted loss from square loss the late exponential part gets worse
# while the early part gets worse.
# For SC training earlier, regularization helped a bit but not much...

Plots.scatter(bgtest.x_grid,Ytrain_ode[:,2],label="data")#,legend=:bottomright)
Plots.plot!(bgtest.x_grid,v_true,label="truth")
Plots.plot!(bgtest.x_grid,test_predict_o1[:,2],label="opt-v1")
Plots.plot!(bgtest.x_grid,test_predict_o4[:,2],label="opt-v1-full",lw=2.5,ls=:dot)
Plots.plot!(bgtest.x_grid,predict(pc)[:,2],label="opt-v1",lw=2.5,ls=:dash)
Plots.title!(raw"$v_{c}$")
Plots.xlabel!(raw"$\log(a)$")
Plots.ylabel!(raw"$v_{c}(a)$")
savefig("../plots/vc_learning_v1_multnoise$(Ïƒfakeode)_Adam$(niter_1)_$(niter_2)_$(niter_3)_$(Î·_1)_$(Î·_2)_$(Î·_3)_bfgs.png")

